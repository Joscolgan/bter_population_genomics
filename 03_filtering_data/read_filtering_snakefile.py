#!/usr/bin/env python3
##############################################################################
##############################################################################
# Author: Joe Colgan                   Program: read_filtering_snakefile.py
#
# Date: 05/03/2015
#
##############################################################################
# Import modules
import os.path
import tempfile

from helper_functions import *

# This script takes two fastq files (i.e. pairs) per sample(s) as input and filters
#  input reads to remove low quality and ambigious sequences for one or more samples.
# Final output is two cleaned fastq files.
#
# To achieve final output, the Snakefile contains custom-defined rules (see below) that
# outline commands to execute sequentially to take custom-defined input(s) and generate
# final output (as defined in rule all).
#
# For this specific script, rules are defined as follows:
# rule all:
#    - Defines the expected final output of the Snakefile.
# rule trim_reads:
#    - For each input, a number of bases (INT) will be removed from left (-b) or from right (-e)
# rule interleave_reads:
#    - Pair-end trimmed sequences will be interleaved into a seperate file per sample.
#    The rule checks if sequence headers for each input pair contain the same header information.
#    If they differ, an error is raused.
# rule filter_merged_reads:
#    - For each sample, interleaved sequences will be filtered based on average base quality.
#    - Sequences must contain at least 75% of bases with a quality score of >=20 to be retained.
#    - Sequences with ambigious 'N' bases will be removed.
# rule combine:
#     - Prior to k-mer counting (a step that takes all samples as input), filtered and merged files
#     are combined together.
# rule count_kmers:
#     - Builds a k-mer count table populated by k-mers (size=31) present within input.
# rule calculate_distribution:
#     - Calculates the abundance distribution of k-mers within input w/ pre-made k-mer count table.
# rule plot_distribution:
#     - Plots k-mer abundance vs count using a custom R script.
# rule filter_kmers:
#     - Initial Snakefile run will fail at this step due to the absence of MIN_FREQ.txt.
#     - MIN_FREQ.txt contains the minimum k-mer abundance to filter and must be populated based off
#     visual examination of plot generated by rule plot_distribution.
#     - Once stated in MIN_FREQ.txt, rule will trim input sequences at minimum k-mer abundace.
# rule remove_truncated_sequences:
#     - Removes truncated k-mer filtered sequences below a specified length (-L)
#     - Drops orphan sequences generated by k-mer filtering steps.
# rule split_cleaned_reads:
#     - Split cleaned interleaved sequences into individual pairs resulting in the generation of
#     two fastq files.


##############################################################################
#
# Sample information
#
##############################################################################
# Bumblebee (Bombus terrestris) males were collected summer 2014
# Sample sites (n=26) were from across the UK
# Each individual and site were assigned unique identifiers
# Example: '2014_Bter_P_D_14_260_head'
# Explanation:{year_collected}_{species}_{site_type}_{sex}_{site_number}_{tube_number}_{tissue_type}
# species: Bter = Bombus terrestris
# site_type: P = Pastoral, M = Mixed, A = Arable
# sex: D = Male (drone)

##############################################################################
#
# Prior to use
#
##############################################################################

# To run read_filtering_snakefile.py:
#  1. Download and install the following software:
#   seqtk (https://github.com/lh3/seqtk)
#   fastx_toolkit (http://hannonlab.cshl.edu/fastx_toolkit/download.html)
#   khmer (https://github.com/dib-lab/khmer)
#
#  2. Ensure helper_functions.py is within the same directory of the Snakefile.
#
#  3. Assign global variables for use within specific rules
#     Please see section below for further information on variable to be assigned
#
#  4. Assignment of wildcards to be used within the rules
#
#  5. Define all the input and outputs of each rule with respect to the above defined wildcards
#
#  6. Each rule willl take assigned input, global variables and generate defined outputs.
#
#  7. Input data should be formatted in the context of defined wildcards: {sample}.{pair}.fastq
#       For example: 2014_Bter_P_D_14_260_head.R1.fastq
#
#  8. Make a text.file called 'sample_list.txt' and put in same directory as Snakfile.
#     Populate 'sample_list.txt' with names of samples to be analysed.
#       For example:
#       2014_Bter_P_D_14_260_head
#       2014_Blap_P_D_21_412_thorax
#       2014_Bpas_A_D_20_391_thorax

##############################################################################
# Running Snakemake
##############################################################################

# To run Snakefile present within current directory
# snakemake -s <Snakefile_name>.py -p -j <number of cores>
# '-s' option is required if Snakefile is not named Snakefile (e.g. read_filtering_snakefile.py)
# '-p' prints shell commands to console during execution of each rule.
# '-j' number of cores. Snakemake can run jobs in parallel.

##############################################################################
##############################################################################
# Assign global variables for use in rules (see below)
##############################################################################
# For rule trim_reads, assign path for input data
RAW_DATA                      = "./input/{sample}/combined/{sample}{pair}.fastq.gz"

# For rule filter_merged_reads specify phred quality score type (33 for Illumina HiSeq reads)
QUAL_PHRED                    = 33

# For rule filter_merged_reads specify minimmum quality score to keep
MIN_QUALITY                   = 20

# For rule filter_merged_reads specify minimum percent of bases that must have minimum quality
PERCENT_BASES                 = 40

N_BASES                       = 15

# For rule count_kmers specify k-mer length for generation of count table
KMER_LENGTH                   = 31

# For rule count_kmers, specify the number of tables to use
NUM_TABLES                    = 10

# For rule count_kmers, specify hashtable size
TABLE_SIZE                    = 8e9

# For rule filter_kmers; Snakemake will use min(MAX_THREADS, --cores)
MAX_THREADS                   = 80

# For rule remove_truncated_sequences specify minimum length of reads to retain post-filtering
MIN_LENGTH                    = 50 # To be calculated based off length of trimmed reads

##############################################################################
# Assignment of wildcards to be used within rules
##############################################################################
# Open file and read in contents - one sample per line
SAMPLES = open('sample_list.txt').read().splitlines()
print(SAMPLES)
for sample in SAMPLES:
    if sample.find('.') != -1:
        raise IOError("Names should use '_', not '.'. Problem with: %s" % sample)

# Assign pair information
PAIR        = ["_1", "_2"]

##############################################################################
# Specify all input/output files in terms of sample wildcards
##############################################################################
# Output adaptor removed data here
ADAPTOR_REMOVED      = "temp/01_adaptor_removed/{sample}.R1.fastq",\
                       "temp/01_adaptor_removed/{sample}.R1.unpaired.fastq",\
                       "temp/01_adaptor_removed/{sample}.R2.fastq",\
                       "temp/01_adaptor_removed/{sample}.R2.unpaired.fastq"

# Output interleaved data here:
INTERLEAVED_DATA          = "results/01_interleaved/{sample}.R1_R2.fastq.gz"

# Output quality filtered data here
FILTERED_DATA              = "results/02_filtered/{sample}.R1_R2.fastq.gz"

# ============================================================================
# Kmer-removal
# ============
# - Combine filtered reads from individual sample libraries
# - Generate hash count table
# - Generate a histogram of "K-mer count vs K-mer frequency" and decide cutoff
# - Use cut-off to filter individual files
# - Drop truncated and single-end reads from K-mer free sequences
# - Split cleaned paired-end data
# ============================================================================
# Output combined interleaved fastq file here
COMBINED_DATA        = "results/03_combined/combined_total.R1_R2.fastq.gz"

# Output k-mer count data here
COUNT_DATA           = "results/04_count/combined.R1_R2.ct"

# Output histogram data here
HISTOGRAM_DATA       = "results/04_count/combined.R1_R2.histo"

# Output plotted .png file here
HISTOGRAM_PLOT       = "temp/07_count/combined.R1_R2.histo.png"

# Output K-mer free paired-end fastq here
KMER_FREE_DATA       = "results/05_kmer_free/{sample}.R1_R2.fastq.gz"

# Output for cleaned data
CLEANED_DATA         = "results/06_length_filtered/{sample}.R1_R2.fastq.gz"

# Output split final cleaned data here
PAIRED_DATA          = "results/07_extract_clean/{sample}.R1_R2.fastq.gz"

# Output single "orphan" reads here
SINGLE_DATA          = "results/07_extract_clean/{sample}.R1_R2_single.fastq.gz"

##Â Output cleaned data here
SPLIT_DATA           = ["results/08_split_clean/{sample}.R1.fastq.gz",
                       "results/08_split_clean/{sample}.R2.fastq.gz"]

##############################################################################
# Define binaries in context of path relative to Snakefile
##############################################################################
# binaries
# Align lines of code using 'Assign Align' using cmd+shift+p and selecting 'align'
# Create dictionaries for directories and  tools'
dirs  = {}
dirs['project'] = os.path.abspath('/ichec/home/users/joscolgan/')
dirs['src']     = os.path.join(dirs['project'], 'bin')

tools = {}
ve_path = 'KhmerEnv/bin/'
print(ve_path)

tools['interleave']           = os.path.join(ve_path, 'interleave-reads.py')
tools['filter_abund']         = os.path.join(ve_path, 'filter-abund.py')
tools['load_into_counting']   = os.path.join(ve_path, 'load-into-counting.py')
tools['abundance_dist']       = os.path.join(ve_path, 'abundance-dist.py')
tools['extract']              = os.path.join(ve_path, 'extract-paired-reads.py')
tools['split']                = os.path.join(ve_path, 'split-paired-reads.py')
check_tools(tools)  # Probably redundant - Snakemake will fail and throw error if can't locate tool

##############################################################################
#
# Specify rules with commands to be executed
#
##############################################################################

# First rule is list the final output
rule all:
    input: expand(KMER_FREE_DATA, sample=SAMPLES, pair=PAIR)
        
# Second rule is to identify and trim adaptors
rule adaptor_removal:
    input:  expand(RAW_DATA, sample=SAMPLES, pair=PAIR)
    output: ADAPTOR_REMOVED
    run:
        check_files_arent_empty(input)
        shell("java -jar {tools[trim]} PE {input[0]} {input[1]} {output[0]} {output[1]}\
               {output[2]} {output[3]} ILLUMINACLIP:combined.TruSeq_adaptors.fa:2:30:10 MINLEN:36")

# Interleave paired-end data and output
rule interleave_reads:
    input: expand("temp/02_trimmed/{{sample}}.{pair}.fastq", pair=PAIR)
    output: MERGED_DATA
    run:
        try:
            if len(input) != 2:
                raise ValueError("Was expecting two input files, got: {input}")
            check_identical_fastq_headers(input[0], input[1])
            shell("{tools[interleave]} {input[0]} {input[1]} -o {output} && [[ -s {output} ]]")
        except ValueError:
            raise IOError("%s does not exist" % (input))
    # seqtk interleaving is incompatible with split-paired-reads.py

# Filter interleaved data by base score and remove ambiguous bases
rule filter_merged_reads:
    input:  ADAPTOR_REMOVED
    output: FILTERED_DATA
    run:
        check_files_arent_empty(input)
        shell("{tools[fastq_quality_filter]} \
              -Q {QUAL_PHRED} \
              -q {MIN_QUALITY} \
              -p {PERCENT_BASES} \
              -i {input[0]} > {output} && [[ -s {output} ]]")

# Combine all sequences
rule combine_samples:
    input: expand("results/02_filtered/{sample}.R1_R2.fastq.gz", sample=SAMPLES)
    output: COMBINED_DATA
    run:
        check_files_arent_empty(input)
        shell("zcat {input} | gzip > {output} && [[ -s {output} ]]")

# Count k-mers using combined data
rule count_kmers:
    input: COMBINED_DATA
    output: COUNT_DATA
    threads: MAX_THREADS
    run:
        check_files_arent_empty(input)
        shell("{tools[load_into_counting]} -k {KMER_LENGTH} -N {NUM_TABLES} -T {threads}"
              " -x {TABLE_SIZE} -b {output} {input} && [[ -s {output} ]]")

# Generate a histogram for k-mer count and k-mer frequency
rule calculate_distribution:
    input: COUNT_DATA, COMBINED_DATA
    output: HISTOGRAM_DATA
    run:
        check_files_arent_empty(input)
        shell("{tools[abundance_dist]} {input[0]} {input[1]} {output} && [[ -s {output} ]]")

# Plot histogram
rule plot_distribution:
    input: HISTOGRAM_DATA
    output: HISTOGRAM_PLOT  # is .png
    run:
        check_files_arent_empty(input)
        shell("Rscript makegraph.R {input} {output} && [[ -s {output} ]]")

# Filter individual samples using k-mer frequency cut-off
rule filter_kmers:
    input: COUNT_DATA, FILTERED_DATA
    output: KMER_FREE_DATA
    threads: MAX_THREADS
    message: "Executing khmer filtering. If it fails, set cut value like: 'echo 15 > MIN_FREQ.txt' \
              after looking at {HISTOGRAM_PLOT}"
    run:
        check_files_arent_empty(input)
        with open('MIN_FREQ.txt') as MIN_FREQ_FILE:
            MIN_FREQ = int(MIN_FREQ_FILE.readlines()[0])
            shell("{tools[filter_abund]}"
                  " -T {MAX_THREADS}"
                  " -C {MIN_FREQ}"
                  " {input[0]}"
                  " {input[1]}"
                  " -o {output} --gzip")

# Drop truncated and single-end reads generated from k-mer filtering step
rule remove_truncated_sequences:
    input: KMER_FREE_DATA
    output: CLEANED_DATA
    run:
        check_files_arent_empty(input)
        shell("zcat {input} | {tools[seqtk]} seq -L {MIN_LENGTH} - | gzip > {output}")

# Split paired-end reads and output
rule extract_paired_cleaned_reads:
    input: CLEANED_DATA
    output: PAIRED_DATA, SINGLE_DATA
    run:
        check_files_arent_empty(input)
        shell("{tools[extract]} -p {output[0]} -s {output[1]} --gzip {input}")

# Split paired-end reads and output
rule split_paired_cleaned_reads:
    input: PAIRED_DATA
    output: SPLIT_DATA
    run:
        check_files_arent_empty(input)
        shell("{tools[split]} -1 {output[0]} -2 {output[1]} --gzip {input}")
