---
title: "2018–01-21_dup_heatmap"
author: "JoeColgan"
date: "21 January 2018"
output: html_document
---

Examination of putative duplications amongst the _Bombus terrestris_ genomes:  
1) Initial duplicated regions were assessed by lumpySV using paired-end and
split-read information.   
2) Using this original coordinates, regions of low complexity (potential repeative regions)
as well as technical artefacts present in the bumblebee genome assembly, such as ambiguous
bases ('N'). Regions with more than 10% base composition of ambiguous bases were removed. 
3) In addition, supporting units (SU) used by lumpySV were plotted and threshold of
quality determined. 
4) Regions were also filtered by size. Size had to be the same as that the shufflebed 
programme was able to calculate (approximately 200kb).  
5) For retained regions, read depth was calculated using samtools depth.  
6) At each locus, read depth for each sample was normalised by total mapped reads. 
7) Normalised read depth was further normalised by locus median read depth to allow for
assessment of variability at each locus. 
8) Read depth supported duplicated loci were compared against randomly chosen loci of similar
size across the genome. 

```{r}
## Load libraries:
libraries <- c("reshape", "ggplot2", "matrixStats", 
               "ggpubr", "gplots", "VariantAnnotation")
for (lib in libraries) {
        if (require(package = lib, character.only = TRUE)) {
                print("Successful")
        } else {
                print("Installing")
                source("https://bioconductor.org/biocLite.R")
                biocLite(pkgs = lib)
                library(lib, character.only = TRUE )
        }
}
```


## Step Two: Load in raw read counts for putative duplicated loci:  
We also load in a file containing sample names. 
We abbreviate each sample name for plotting purposes. 

```{r}
## Set path to working directory
#setwd("/Volumes/apocrita/autoScratch/2016-10-11_Bombus_population_genomics/results/2017-11-02_terrestris_combined_reanalysis/results/temp/09_final_clean/2017-11-09_speedseq/raw_median_temp/06_final/")

## Read in file containing median read depth of putative duplications:
duplications <- read.table("final_combined.median_depth.all_dups.n41.txt")

## Rename the row names to include information on genomic locus co-ordinates using first column
rownames(duplications) <- paste(as.character(duplications$V1), sep="")
duplications <- duplications[,-c(1)]  ## Remove this column 

## Read in file containing sample list:
samples <- scan("./sample_names.n41.txt", as.character())

## Rename samples:
## Renaming the sample.ids
new_names <- list()

## Create a list:
for (name in samples){
        new_names[name] <- paste(#strsplit(name, "_")[[1]][3], "_",
                                 strsplit(name, "_")[[1]][5], "_",
                                 strsplit(name, "_")[[1]][6], sep="")
}

## Unlist as a character string and update sample ids:
samples <- as.character(unlist(new_names))

## Add colnames
colnames(duplications) <- samples
```

## Step Three: Normalise raw read depth by total number of mapped reads:

```{r}
## Subset raw read depth counts:
duplications_count_nums <- duplications[,1:41]

## Read in total mapped reads:
total_read_numbers <- read.table("mapped_total_reads.n41.samples.txt", header=FALSE)

## Add column names similar to experimental data:
colnames(total_read_numbers) <- colnames(duplications_count_nums)

## Make counts normalized according to total # of reads for that library
duplications_count_nums_norm_tmp <- duplications_count_nums
for (sample in colnames(duplications)) {
        duplications_count_nums_norm_tmp[[sample]] <- duplications_count_nums[[sample]] / total_read_numbers[[sample]]
}

```

## Step Four: Normalise raw read depth by median read depth to allow within locus comparison:

```{r}
## Calculate median read depth per locus:
duplications_count_nums_norm_tmp_rowMedians <- rowMedians(as.matrix(duplications_count_nums_norm_tmp))

## Combine normalised counts with row medians
duplications_count_nums_norm_tmp_plus_rowMedians <- cbind(duplications_count_nums_norm_tmp, duplications_count_nums_norm_tmp_rowMedians)

## normalise between loci - to pull out things that vary only wihtin our dataset - not compared to reference - this way we just see fold changes.
counts_normalized <- duplications_count_nums_norm_tmp_plus_rowMedians
for (locus in rownames(duplications_count_nums_norm_tmp_plus_rowMedians)) {
        counts_normalized[locus,] <-duplications_count_nums_norm_tmp_plus_rowMedians[locus,] / as.numeric(duplications_count_nums_norm_tmp_plus_rowMedians[locus,"duplications_count_nums_norm_tmp_rowMedians"])
}
rm(duplications_count_nums_norm_tmp) # no longer needed

## Remove the rowMedians counts column
counts_normalized$duplications_count_nums_norm_tmp_rowMedians <- NULL
```

## Step Five: Calculate standard deviation for each locus:
For each row, standard deviation was calculated to examine variability between samples. Sites were ranked and plotted by standard deviation to examine the most variable sites within the dataset. 

```{r}
## Calculate standard deviation for each row 
counts_normalized_sd <- apply(counts_normalized, MARGIN=1, FUN=sd)
counts_normalized_sd_sorted <- counts_normalized_sd[order(counts_normalized_sd)]
plot(counts_normalized_sd_sorted, 
     xlab = "CNV bins", 
     ylab = "Standard deviation", 
     main = "Variance amongst LumpySV called bins")

## Sort by standard deviation
## DOESN'T SORT APPROPRIATELY - SORTS BY HEADER RATHER THAN NUMERIC VALUE
## counts.normalized.sdsorted <- counts.normalized[order(counts.normalized.sd),]
## One approach is to add the standard deviation values to the dataframe containing the normalized counts
counts_normalized_plus_sd <- cbind(counts_normalized, counts_normalized_sd)

## Sort by standard deviation
counts_normalized_plus_sd_sorted <- counts_normalized_plus_sd[order(-counts_normalized_sd),]

## Subset the top 20 rows with greatest standard deviation
counts_normalized_plus_sd_sorted_top20 <- head((counts_normalized_plus_sd_sorted), n=20)

## Remove coordinates and counts.normalized.sd from the dataframe
counts_normalized_plus_sd_sorted_top20$counts_normalized_sd <- NULL
```

## Step Six: Generate heatmap of log transformed normalised read depth:
The data was log2 transformed. To allow for visualisation with ggplot2, the data was transposed and reformatted using melt().

```{r}
## Log2 transfrom data:
counts_normalized_plus_sd_sorted_top20_log_transformed <-as.matrix(log2(counts_normalized_plus_sd_sorted_top50))

## Transpose the normalised matrix and output as a dataframe
## Then melt to allow for visualisation using ggplot2
counts_normalized_plus_sd_sorted_top20_log_transformed_trans <- t(counts_normalized_plus_sd_sorted_top20_log_transformed)

df_melt <- melt(counts_normalized_plus_sd_sorted_top20_log_transformed_trans)

## Rename colnames for melted.dataframe:
colnames(df_melt) <- c("sample_name", "CNV_coordinates", "Log_fold_change")

## Reorder levels for plotting:
df_melt$CNV_coordinates <- factor(df_melt$CNV_coordinates, levels=as.character(unlist(df_melt$CNV_coordinates)))

df_melt$sample_name <- factor(df_melt$sample_name, levels=as.character(unlist(df_melt$sample_name)))

## Generation of a heatmap for visualisation:
heatmap <- ggplot(df_melt, aes(sample_name, CNV_coordinates)) +
  geom_tile(aes(fill = Log_fold_change), color = "black") +
  scale_fill_gradient2(low = "yellow", mid="white", high = "blue") +
  ylab("CNV genomic coordinates") +
  xlab("Samples") +
        theme(legend.title = element_text(size = 15, face="bold"),
              legend.text = element_text(size = 15, face="bold"),
              axis.text.x = element_text(angle = 45, hjust = 1, size=8, face="bold"),
              axis.text.y= element_text(size=8, face="bold"),
             axis.title=element_text(size=14,face="bold")) +
        theme(legend.position="top") +
        scale_y_discrete(position = "right")

```

## Step Seven: Comparison of experimental data against randomly chosen genome-wide data:

```{r}
## Read in read counts for randomly chosen throughout the bumblebee genome:
random_counts <- read.table("../../combined_median_depth.all_dups.n845.n41.txt")

## Rename the row names to include information on genomic locus co-ordinates using first column
rownames(random_counts) <- paste(as.character(random_counts$V1))
random_counts <- random_counts[,-c(1)]  ## Remove this column 

## Add colnames:
colnames(random_counts) <- colnames(duplications[1:41])

# Remove the 'length' column from the deletions dataframe and replace with 'SV' status of duplication
duplications$SV     <- c("duplication")
random_counts$SV    <- c("random") 

## Now combine both dataframes containing deletions and random counts:
combined_counts <- rbind(duplications, random_counts)

## Normalise experimental dataset using number of reads aligned to reference genome (ie. library size)
combined_counts_normalized.tmp <- combined_counts[,1:41]
for (sample in colnames(combined_counts[,1:41])) {
        combined_counts_normalized.tmp[[sample]] <- combined_counts_normalized.tmp[[sample]] / total_read_numbers[[sample]]
}

combined_counts_normalized.tmp <- cbind(rownames(combined_counts_normalized.tmp), combined_counts_normalized.tmp)

combined_counts_normalized.tmp$SV <- combined_counts$SV
combined_counts_normalized.tmp$SV <- NULL

## Reshape normalised counts and name ids by genomic coordinates
combined_counts_normalized.tmp.melt <- (melt(combined_counts_normalized.tmp, id.var=1))

## Rename columns for reshaped dataframe
colnames(combined_counts_normalized.tmp.melt) <- c("Coordinates", "Sample", "Normalised_read_depth")

```

## Step Eight: Identification of sites with elevated read depth:
 
Using the normalised median read depth per randomly chosen site:
- Median values per calculated per site (row)
- Mean and standard deviation was then calculated using all sites

**_Chebyshev's inequality_**  
In probability theory, Chebyshev's inequality guarantees that, for a wide class of probability
distributions, "nearly all" values are close to the mean - the precise st

k	Min. % within k standard    Max. % beyond k standard
        deviations of mean	    deviations from mean  
        
1	0%	                        100%
√2	50%	                        50%
1.5	55.56%	                        44.44%
2	75%	                        25%
3	88.8889%	                11.1111%
4	93.75%	                        6.25%
5	96%	                        4%
6	97.2222%	                2.7778%
7	97.9592%	                2.0408%
8	98.4375%	                1.5625%
9	98.7654%	                1.2346%
10	99%	                        1%

```{r}
## Normalise experimental dataset using number of reads aligned to reference genome (ie. library size)
## Combine random and experimental medians and plot
combined_counts_normalized.tmp$SV <- combined_counts$SV

combined_counts_normalized.tmp_melt <- melt(combined_counts_normalized.tmp)

##Basic plot of points
## PLotting density - raw standard deviation
## Subset back into random and deletions:
random.melted       <-  combined_counts_normalized.tmp_melt[combined_counts_normalized.tmp_melt$SV=="random",]
duplications.melted <-  combined_counts_normalized.tmp_melt[combined_counts_normalized.tmp_melt$SV=="duplication",]

## Calculate median:
random.melted.median       <- median(random.melted$value)
duplications.melted.median <- median(duplications.melted$value)

## Calculate the standard deviation for random sites
random.mean <- mean(random.melted$value) 
sd1 <- (sd(random.melted$value) + random.mean) 
sd2 <- (sd(random.melted$value) * 2) + random.mean 
sd3 <- (sd(random.melted$value) * 3) + random.mean 
sd4 <- (sd(random.melted$value) * 4) + random.mean

## Plot a density plot:
den_plot<- ggplot(combined_counts_normalized.tmp_melt, aes(value, color=SV, fill=SV)) +
        geom_density(aes(color=SV, fill=SV), alpha=0.5) + 
        xlim(0,10e-07) + 
        xlab("Normalised read depth") +
        geom_vline(xintercept =  sd3, colour = 'blue', linetype = "longdash") +
        geom_vline(xintercept =  random.mean, colour = 'black', linetype = "longdash")  +
        theme_bw() +
        scale_color_manual(values=c("black", "black")) +
        scale_fill_manual(values=c("black", "grey")) +
        theme(legend.title = element_text(size = 15, face="bold"),
              legend.text = element_text(size = 15, face="bold"),
              axis.text.x = element_text(size=15),
              axis.text.y= element_text(size=15),
             axis.title=element_text(size=15,face="bold")) +
        theme(legend.position="top")
        
## Print to console:        
den_plot 

## How many sites > greater than 3 standard deviations from the mean?
## For the random sites:
nrow(combined_counts_normalized.tmp_melt[which(combined_counts_normalized.tmp_melt$SV == "duplication"), ]) # 11193

potential_sig_dups <- combined_counts_normalized.tmp_melt[which(combined_counts_normalized.tmp_melt$SV == "duplication" & combined_counts_normalized.tmp_melt$value > sd3), ] #2444

nrow(combined_counts_normalized.tmp_melt[which(combined_counts_normalized.tmp_melt$SV == "random" & combined_counts_normalized.tmp_melt$value > sd3), ]) #174

nrow(combined_counts_normalized.tmp_melt[which(combined_counts_normalized.tmp_melt$SV == "random"), ]) #34645

## Revised examination:
# 3 standard deviation accounts for 78.17% of the experimental sites
# 3 standard deviation acocunts for 99.49% of the randomly chosen sites

## For the significant duplicated sites:
## 1) Number of unique sites: 133
length(unique(potential_sig_dups$`rownames(combined_counts_normalized.tmp)`))

## 2) Number of duplications within samples per site:
sort(table(potential_sig_dups$`rownames(combined_counts_normalized.tmp)`))
```

## Step Eight: Identification of the number of sites per individual falling within duplicated regions:

```{r}
## Subset the number of putatively duplicated sites with RD higher than 2 standard deviation from mean of randomly chosen sites.
highRD <-subset(melted, SV == "duplication" & value > sd3)

highRD$`rownames(combined_counts_normalized.tmp)` <- as.character(unlist(highRD$`rownames(combined_counts_normalized.tmp)`))

## Change the column names
colnames(highRD) <- c("site", "SV", "sample", "RD")

## Identification of the number of sites with at least one individual that has a RD higher than 3 standard deviations:
length(unique(highRD$site)) #133 sites

# Identification of samples per site
# For example how many sites have high RD with five individuals at this site
counts <- table(highRD$site)
length(counts)

## 133 sites with at least one individual with more than 3 standard deviations from the random mean:
highRD_sites<-unique(highRD$site)
highRD_sites_vector<-as.character(highRD_sites)

## Examine the number of individuals with reduced median read depth
l=list()
for (name in highRD_sites){
        count <- nrow(melted[which(melted$SV == "duplication" & melted$RD > sd3  & melted$site == name), ])
        info <- melted[which(melted$SV == "duplication" & melted$RD > sd3  & melted$site == name), ]
        if (count ==  41) {
                print(head(as.character(info$site), n=1))
                }
}

## There appears to be a problem with the use of 
highRD_counts_table <- sort(round((table(as.character(highRD$site)))/41, 2))

highRD_counts_table_df <- as.data.frame(highRD_counts_table)

## Plot frequency of duplications:
dup_freq_hist<- qplot(highRD_counts_table_df$Freq,
                      geom="histogram",
                      binwidth = 0.05,  
                      xlab = "Frequency",  
                      fill=I("grey"), 
                      col=I("black"), 
                      alpha=I(.2)) +
                        theme_bw()

## Subset conserved sites (found in all individuals):
highRD.counts.table.df<- highRD.counts.table.df[highRD.counts.table.df$Freq==1,]

## Write low RD sites to file:
write.table(highRD_sites_vector, file="highRD_sites_vector.n133.dups.txt", col.names = F, row.names = F, sep = "\t")

write.table(highRD.counts.table.df, file="conserved.highRD_sites.n22.dups.txt", col.names = F, row.names = F, sep = "\t")
```
